"""
Ingest all spiritual text datasets and create embeddings for RAG pipeline
Handles multiple data formats: CSV, JSON with various structures
"""
import os
import sys
import json
import csv
import logging
from pathlib import Path
from typing import List, Dict
import numpy as np

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Add parent directory to path
sys.path.append(str(Path(__file__).parent.parent))

from config import settings

# Try to import sentence transformers
try:
    from sentence_transformers import SentenceTransformer
    EMBEDDING_AVAILABLE = True
except ImportError:
    EMBEDDING_AVAILABLE = False
    logger.warning("sentence-transformers not available. Install with: pip install sentence-transformers")


class UniversalScriptureIngester:
    """Ingest and process all spiritual text datasets"""

    def __init__(self):
        self.raw_data_dir = Path(__file__).parent.parent / "data" / "raw"
        self.processed_data_dir = Path(__file__).parent.parent / "data" / "processed"
        self.processed_data_dir.mkdir(parents=True, exist_ok=True)

        # Initialize embedding model if available
        self.embedding_model = None
        if EMBEDDING_AVAILABLE:
            try:
                logger.info(f"Loading embedding model: {settings.EMBEDDING_MODEL}")
                self.embedding_model = SentenceTransformer(settings.EMBEDDING_MODEL)
                logger.info("Embedding model loaded successfully")
            except Exception as e:
                logger.error(f"Failed to load embedding model: {e}")

    def find_dataset_files(self) -> List[Path]:
        """Find all dataset files in raw data directory"""
        files = []

        if not self.raw_data_dir.exists():
            logger.error(f"Raw data directory not found: {self.raw_data_dir}")
            return files

        # Look for CSV and JSON files
        csv_files = list(self.raw_data_dir.glob("*.csv"))
        json_files = list(self.raw_data_dir.glob("*.json"))

        files.extend(csv_files)
        files.extend(json_files)

        logger.info(f"Found {len(files)} data files")
        return files

    def parse_csv_file(self, file_path: Path) -> List[Dict]:
        """Parse CSV file and extract verses"""
        verses = []

        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)

                limit = 10000
                if "gita" in file_path.name.lower():
                    limit = 50000 # Ensure all Gita verses are included

                for idx, row in enumerate(reader):
                    if idx > limit:
                        break

                    verse = self._extract_verse_from_csv_row(row, file_path.stem)
                    if verse:
                        verses.append(verse)

            logger.info(f"âœ“ Parsed {len(verses)} verses from {file_path.name}")

        except Exception as e:
            logger.error(f"âœ— Error parsing CSV {file_path.name}: {e}")

        return verses

    def parse_json_file(self, file_path: Path) -> List[Dict]:
        """Parse JSON file and extract verses"""
        verses = []

        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            # Handle different JSON structures
            if isinstance(data, list):
                for idx, item in enumerate(data):
                    if idx > 10000:  # Limit for performance
                        break
                    verse = self._extract_verse_from_dict(item, file_path.stem)
                    if verse:
                        verses.append(verse)
            elif isinstance(data, dict):
                # Could be nested structure
                for key, value in data.items():
                    if isinstance(value, list):
                        for idx, item in enumerate(value):
                            if idx > 10000:  # Limit for performance
                                break
                            verse = self._extract_verse_from_dict(item, file_path.stem)
                            if verse:
                                verses.append(verse)
                    elif isinstance(value, dict):
                        verse = self._extract_verse_from_dict(value, file_path.stem)
                        if verse:
                            verses.append(verse)

            if verses:
                logger.info(f"âœ“ Parsed {len(verses)} verses from {file_path.name}")
            else:
                logger.warning(f"âš  No verses extracted from {file_path.name}")

        except Exception as e:
            logger.error(f"âœ— Error parsing JSON {file_path.name}: {e}")

        return verses

    def _extract_verse_from_csv_row(self, row: Dict, source: str) -> Dict:
        """Extract verse from CSV row"""
        verse = {}

        # Map common column names (case-insensitive)
        row_lower = {k.lower(): v for k, v in row.items() if v}

        # Extract fields
        verse['chapter'] = row_lower.get('chapter') or row_lower.get('adhyaya') or row_lower.get('id', '').split('.')[0] if '.' in row_lower.get('id', '') else None
        verse['verse'] = row_lower.get('verse') or row_lower.get('shloka') or row_lower.get('shloka_number')
        
        # Priority for English content
        verse['text'] = row_lower.get('engmeaning') or row_lower.get('translation') or row_lower.get('english') or row_lower.get('text')
        
        # Original Sanskrit/Hindi
        verse['sanskrit'] = row_lower.get('shloka') or row_lower.get('original') or row_lower.get('sanskrit')
        
        verse['transliteration'] = row_lower.get('transliteration') or row_lower.get('iast')
        
        # Add meaning/explanation
        verse['meaning'] = row_lower.get('meaning') or row_lower.get('explanation') or row_lower.get('wordmeaning') or row_lower.get('hinmeaning')
        verse['hindi'] = row_lower.get('hinmeaning') or row_lower.get('hindi')

        # Only return if we have essential fields
        if verse.get('chapter') and verse.get('verse') and verse.get('text'):
            verse['scripture'] = self._infer_scripture(source)
            verse['source'] = source
            verse['reference'] = f"{verse['scripture']} {verse['chapter']}.{verse['verse']}"
            verse['language'] = 'en'
            verse['topic'] = self._infer_topic(verse)

            return {k: v for k, v in verse.items() if v}  # Remove None values

        return None

    def _extract_verse_from_dict(self, item: Dict, source: str) -> Dict:
        """Extract verse from dictionary"""
        if not isinstance(item, dict):
            return None

        verse = {}
        item_lower = {k.lower(): v for k, v in item.items() if v is not None}

        # Extract fields with type checking
        def safe_get(key_list):
            for key in key_list:
                if key in item_lower:
                    val = item_lower[key]
                    if isinstance(val, (str, int)):
                        return str(val).strip() if isinstance(val, str) else str(val)
            return None

        verse['chapter'] = safe_get(['chapter', 'adhyaya', 'book', 'mandala', 'kaanda'])
        verse['verse'] = safe_get(['verse', 'shloka', 'shloka_number', 'verse_number'])
        verse['text'] = safe_get(['text', 'translation', 'english', 'meaning', 'content'])
        verse['sanskrit'] = safe_get(['shloka', 'sanskrit', 'original', 'devanagari'])
        verse['transliteration'] = safe_get(['transliteration', 'iast', 'romanized', 'transliteraion'])

        # Only return if we have essential fields
        if verse.get('chapter') and verse.get('verse') and verse.get('text'):
            verse['scripture'] = self._infer_scripture(source)
            verse['source'] = source
            verse['reference'] = f"{verse['scripture']} {verse['chapter']}.{verse['verse']}"
            verse['language'] = 'en'
            verse['topic'] = self._infer_topic(verse)

            return {k: v for k, v in verse.items() if v}  # Remove None values

        return None

    def _infer_scripture(self, source: str) -> str:
        """Infer scripture name from filename"""
        source_lower = source.lower()

        if 'bhagavad' in source_lower or 'bhagwad' in source_lower or 'gita' in source_lower:
            return 'Bhagavad Gita'
        elif 'mahabharata' in source_lower:
            return 'Mahabharata'
        elif 'ramayana' in source_lower or 'balakanda' in source_lower or 'ayodhya' in source_lower:
            return 'Ramayana'
        elif 'rigveda' in source_lower:
            return 'Rig Veda'
        elif 'atharvaveda' in source_lower:
            return 'Atharva Veda'
        elif 'yajurveda' in source_lower or 'vajasneyi' in source_lower:
            return 'Yajur Veda'
        elif 'vedas' in source_lower:
            return 'Vedas'
        else:
            return 'Sanatan Scriptures'

    def _infer_topic(self, verse: Dict) -> str:
        """Infer topic from verse content"""
        text = (verse.get('text', '') + ' ' + verse.get('meaning', '') + ' ' + verse.get('sanskrit', '')).lower()

        topics = {
            'Karma Yoga': ['action', 'duty', 'work', 'karma', 'perform', 'karm'],
            'Bhakti Yoga': ['devotion', 'love', 'surrender', 'worship', 'bhakti', 'prem'],
            'Jnana Yoga': ['knowledge', 'wisdom', 'understand', 'jnana', 'learning', 'gyan'],
            'Mind Control': ['mind', 'control', 'meditation', 'focus', 'discipline', 'mana'],
            'Soul': ['soul', 'atman', 'self', 'eternal', 'immortal', 'aatma'],
            'Equanimity': ['equal', 'balance', 'neutral', 'steady', 'sama', 'equanimity'],
            'Fear': ['fear', 'afraid', 'courage', 'fearless', 'bhaya'],
            'Death': ['death', 'mortality', 'rebirth', 'reincarnation', 'mrutyu'],
            'Liberation': ['liberation', 'moksha', 'freedom', 'enlightenment', 'mukt'],
            'Dharma': ['dharma', 'righteousness', 'duty', 'moral', 'dharm'],
            'Truth': ['truth', 'satya', 'honest', 'real'],
            'Wealth': ['wealth', 'money', 'prosperity', 'success'],
            'Love': ['love', 'affection', 'compassion', 'prem', 'sneh'],
            'War': ['war', 'battle', 'fight', 'yuddh', 'yudh'],
        }

        for topic, keywords in topics.items():
            if any(keyword in text for keyword in keywords):
                return topic

        return 'Spiritual Wisdom'

    def generate_embeddings(self, verses: List[Dict]) -> np.ndarray:
        """Generate embeddings for all verses"""
        if not self.embedding_model:
            logger.warning("âš  No embedding model available - using dummy embeddings")
            return np.zeros((len(verses), 768))

        texts = []
        for verse in verses:
            # Combine fields for better semantic representation
            text_parts = [
                verse.get('text', ''),
                verse.get('sanskrit', ''),
                verse.get('meaning', ''),
            ]
            combined_text = ' '.join([p for p in text_parts if p])
            texts.append(combined_text[:1000])  # Limit length

        logger.info(f"Generating embeddings for {len(texts)} verses...")
        embeddings = self.embedding_model.encode(texts, convert_to_tensor=False, show_progress_bar=True)

        logger.info(f"âœ“ Generated embeddings shape: {embeddings.shape}")
        return embeddings

    def save_processed_data(self, verses: List[Dict], embeddings: np.ndarray):
        """Save processed verses and embeddings"""
        output_file = self.processed_data_dir / "all_scriptures_processed.json"

        # Convert embeddings to list for JSON serialization
        for i, verse in enumerate(verses):
            verse['embedding'] = embeddings[i].tolist()

        # Save as JSON
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump({
                'verses': verses,
                'metadata': {
                    'total_verses': len(verses),
                    'embedding_dim': len(embeddings[0]) if len(embeddings) > 0 else 0,
                    'embedding_model': settings.EMBEDDING_MODEL,
                    'scriptures': sorted(list(set(v.get('scripture', 'Unknown') for v in verses)))
                }
            }, f, ensure_ascii=False, indent=2)

        logger.info(f"âœ“ Saved processed data to {output_file}")

        # Save verses without embeddings for easy inspection
        verses_only_file = self.processed_data_dir / "all_scriptures_verses.json"
        with open(verses_only_file, 'w', encoding='utf-8') as f:
            verses_copy = [{k: v for k, v in verse.items() if k != 'embedding'} for verse in verses]
            json.dump(verses_copy, f, ensure_ascii=False, indent=2)

        logger.info(f"âœ“ Saved verses (without embeddings) to {verses_only_file}")

        # Create index by scripture
        by_scripture = {}
        for verse in verses:
            scripture = verse.get('scripture', 'Unknown')
            if scripture not in by_scripture:
                by_scripture[scripture] = []
            by_scripture[scripture].append(verse)

        index_file = self.processed_data_dir / "scripture_index.json"
        with open(index_file, 'w', encoding='utf-8') as f:
            index_data = {scripture: len(verses) for scripture, verses in by_scripture.items()}
            json.dump(index_data, f, indent=2)

        logger.info(f"âœ“ Saved scripture index to {index_file}")

    def ingest_all(self):
        """Main ingestion pipeline"""
        logger.info("\n" + "=" * 80)
        logger.info("ğŸš€ STARTING UNIVERSAL SCRIPTURE DATASET INGESTION")
        logger.info("=" * 80)

        # Find dataset files
        files = self.find_dataset_files()

        if not files:
            logger.error("\nâŒ No dataset files found!")
            logger.error(f"ğŸ“ Expected location: {self.raw_data_dir}")
            return

        # Parse all files
        all_verses = []
        scripture_counts = {}

        logger.info(f"\nğŸ“‚ Processing {len(files)} files...\n")

        for file_path in sorted(files):
            if file_path.suffix == '.csv':
                verses = self.parse_csv_file(file_path)
            elif file_path.suffix == '.json':
                verses = self.parse_json_file(file_path)
            else:
                logger.warning(f"âš  Unsupported file type: {file_path.name}")
                continue

            if verses:
                all_verses.extend(verses)
                scripture = self._infer_scripture(file_path.stem)
                scripture_counts[scripture] = scripture_counts.get(scripture, 0) + len(verses)

        if not all_verses:
            logger.error("\nâŒ No verses extracted from dataset!")
            return

        logger.info(f"\nâœ… Successfully parsed {len(all_verses)} total verses")
        logger.info("\nBreakdown by scripture:")
        for scripture, count in sorted(scripture_counts.items(), key=lambda x: -x[1]):
            logger.info(f"  â€¢ {scripture}: {count} verses")

        # Remove duplicates based on reference
        unique_verses = {}
        for verse in all_verses:
            ref = verse.get('reference')
            if ref not in unique_verses:
                unique_verses[ref] = verse

        all_verses = list(unique_verses.values())
        logger.info(f"\nâœ… {len(all_verses)} unique verses after deduplication")

        # Generate embeddings
        logger.info("\nğŸ”„ Generating embeddings...")
        embeddings = self.generate_embeddings(all_verses)

        # Save processed data
        logger.info("\nğŸ’¾ Saving processed data...")
        self.save_processed_data(all_verses, embeddings)

        logger.info("\n" + "=" * 80)
        logger.info("âœ… INGESTION COMPLETE!")
        logger.info("=" * 80)
        logger.info(f"ğŸ“Š Total verses processed: {len(all_verses)}")
        logger.info(f"ğŸ“ Output directory: {self.processed_data_dir}")
        logger.info(f"ğŸ“„ Files created:")
        logger.info(f"   â€¢ all_scriptures_processed.json (with embeddings)")
        logger.info(f"   â€¢ all_scriptures_verses.json (without embeddings)")
        logger.info(f"   â€¢ scripture_index.json (count by scripture)")
        logger.info("\nğŸ¯ Data is ready for RAG pipeline!")


def main():
    """Run ingestion"""
    ingester = UniversalScriptureIngester()
    ingester.ingest_all()


if __name__ == "__main__":
    main()
